{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Now that we have explored our data, and cleaned it up into an acceptable format, we are ready to run some modeling techniques on it. We will begin by building multiple generic models on the data set to see how well it performs. After this, we will build a Neural Network to see if we can improve the performance from the baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset\n",
    "\n",
    "This dataset is already cleaned up and preprocessed. It is expected that this dataset has the following columns:\n",
    "* SALEPRICE\n",
    "* PROPERTYZIP\n",
    "\n",
    "> NOTE/TODO: Currently, we don't do anything with the PROPERTYZIP. We plan to build a model for each zipcode, but that is pending our preliminary results (to see if we need to even attempt such a thing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "data = pd.read_csv('output_total_df.csv')\n",
    "\n",
    "# Renaming columns to not include brackets, spaces, or commas\n",
    "column_mapping = {}\n",
    "for col in list(data.columns):\n",
    "\n",
    "    new_col = col.replace(']', ')')\n",
    "    new_col = new_col.replace('[', '(')\n",
    "    new_col = new_col.replace(', ', '-')\n",
    "    new_col = new_col.replace(' ', '')\n",
    "    column_mapping[col] = new_col\n",
    "\n",
    "data = data.rename(columns=column_mapping)\n",
    "\n",
    "# Drop Nulls\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Split\n",
    "\n",
    "We will split our dataset into three sets:\n",
    "* Training - (77%)\n",
    "* Validation - (16.5%)\n",
    "* Testing - (16.5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape:  (96255, 19)\n",
      "Validation Shape:  (23705, 19)\n",
      "Testing Shape:  (23705, 19)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(['Unnamed:0', 'SALEPRICE'], axis=1)\n",
    "Y = data['SALEPRICE']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training Shape: \", X_train.shape)\n",
    "print(\"Validation Shape: \", X_val.shape)\n",
    "print(\"Testing Shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Modeling\n",
    "We will build some standard models on our data and see which model performs the best. The following models will be implemented:\n",
    "\n",
    "* KNearest Neighbors\n",
    "* Support Vector Machine\n",
    "* Gradient Boosted Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Function\n",
    "Here we will define a function that accepts a model, parameters, and data. This model will build the model and test it.\n",
    "\n",
    "This function will be useful for testing all of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "def test_model(model, params, model_name, X_train, y_train, X_test, Y_test):\n",
    "    \"\"\"Trains model, and evaluates it.\n",
    "        PARAMS:\n",
    "            model_choice - SKLearn Model: Model to be trained\n",
    "            params - dictionary: Dictionary of parameters to feed the model\n",
    "            X_train - DataFrame: Training Data, Features\n",
    "            y_train - DataFrame: Training Data, Targets\n",
    "            X_test - DataFrame: Testing Data, Features\n",
    "            y_test - DataFrame: Testing Data, Targets\n",
    "        \n",
    "        RETURNS:\n",
    "            Accuracy - float: Accuracy for specified model and parameters. \n",
    "    \"\"\"\n",
    "    print(\"Begin \", model_name)\n",
    "    start = time.time()\n",
    "\n",
    "    # Run model and get predictions and accuracy\n",
    "    clf = model(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = clf.predict(X_test)\n",
    "    rmse = mean_squared_error(Y_test, predictions) ** 0.5\n",
    "\n",
    "    print(f\"{model_name} - RMSE: \", rmse)\n",
    "    \n",
    "    print(\"Time Elapsed: \", time.time() - start)\n",
    "    \n",
    "    return (rmse, clf, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Models\n",
    "We will run all of the following models through the testing function:\n",
    "* KNearestNeighbors\n",
    "* Linear Support Vector Machine\n",
    "* GradientBoostingRegressor\n",
    "\n",
    "Each of these models will be trained on the `Training Set` and evaluated on the `Validation Set`. The model that performs the best on the evaluation set (lowest `RMSE`), will be promoted to the next phase of model tuning (Hyper Parameter Tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin  KNeighbors\n",
      "KNeighbors - RMSE:  67779.15061106875\n",
      "Time Elapsed:  30.34240436553955\n",
      "Begin  Support Vector Machine\n",
      "Support Vector Machine - RMSE:  57197.77069645129\n",
      "Time Elapsed:  0.3240478038787842\n",
      "Begin  GradientBoostingRegressor\n",
      "GradientBoostingRegressor - RMSE:  6336.435687459149\n",
      "Time Elapsed:  22.934399127960205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "knc_params = {}\n",
    "svr_params = {}\n",
    "gbr_params = {}\n",
    "\n",
    "models_to_test = [\n",
    "    (KNeighborsClassifier, knc_params, \"KNeighbors\"),\n",
    "    (LinearSVR, svr_params, \"Support Vector Machine\"),\n",
    "    (GradientBoostingRegressor, gbr_params, \"GradientBoostingRegressor\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "for model in models_to_test:\n",
    "    results += [test_model(model[0], model[1], model[2], X_train, y_train, X_val, y_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6336.435687459149, GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
      "                          init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                          n_iter_no_change=None, presort='deprecated',\n",
      "                          random_state=None, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False), 'GradientBoostingRegressor'), (57197.77069645129, LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
      "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
      "          random_state=None, tol=0.0001, verbose=0), 'Support Vector Machine'), (67779.15061106875, KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform'), 'KNeighbors')]\n"
     ]
    }
   ],
   "source": [
    "# Sort results by the best accuracy.\n",
    "results.sort(key = lambda x: x[0], reverse=False)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning\n",
    "\n",
    "The `GradientBoostedRegressor` had the best performance.\n",
    "\n",
    "We will now run a `Randomized Grid Search` on it to see which parameters might be most effective. We choose to run Randomized Grid Search instead of a pure Grid Search because running a pure Grid Search would take too much time. In the future, we may want to consider expanding the grid, and trying all combinations.\n",
    "\n",
    "> NOTE: Because we are running a Randomized Grid Search, we will likely not get the best results. However, because of it's random nature, we are likely to get a 'good' result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed: 10.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:  744.9654760360718\n",
      "Best parameters:  {'subsample': 0.9, 'n_estimators': 200, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': None, 'max_depth': 7, 'learning_rate': 0.2, 'alpha': 0.95}\n",
      "Best Score:  0.9981689242028852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "parameters = { \n",
    "    'learning_rate':[0.01,0.1,0.2],\n",
    "    'n_estimators':[50,100,150,200,250,300],\n",
    "    'subsample':[0.75,0.9,1.0],\n",
    "    'min_samples_split':[2,4,6],\n",
    "    'min_samples_leaf':[1,3,5,7],\n",
    "    'max_depth':[3,5,7],\n",
    "    'max_features':(None,'sqrt','log2'),\n",
    "    'alpha':[0.85,0.9,0.95]\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "clf_rand = RandomizedSearchCV(GradientBoostingRegressor(), parameters, n_iter=10, n_jobs=4, verbose=10)\n",
    "\n",
    "clf_rand.fit(X_train, y_train)\n",
    "\n",
    "print(\"Time Elapsed: \", time.time() - start)\n",
    "\n",
    "print(\"Best parameters: \", clf_rand.best_params_)\n",
    "print(\"Best Score: \", clf_rand.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model\n",
    "\n",
    "We will evaluate the performance of the `GradientBoostedRegressor` model on the \n",
    "* Training Set\n",
    "* Validation Set\n",
    "* Testing Set\n",
    "\n",
    "Understanding how the RMSE varies within each of the data sets will help us understand how well the model is generalizing to new data.\n",
    "\n",
    "> NOTE: We will only using the results on the `testing` for determining to accept our decline our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:  108.729012966156\n",
      "Training Root Mean Squared Error:  3237.011572304024\n",
      "Validation Root Mean Squared Error:  4413.472947325979\n",
      "Testing Root Mean Squared Error:  4780.844198604225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "clf_gbr = GradientBoostingRegressor(**clf_rand.best_params_).fit(X_train, y_train)\n",
    "\n",
    "print(\"Time Elapsed: \", time.time() - start)\n",
    "\n",
    "predictions = clf_gbr.predict(X_train)\n",
    "rmse = mean_squared_error(y_train, predictions) ** 0.5\n",
    "print(\"Training Root Mean Squared Error: \", rmse)\n",
    "\n",
    "predictions = clf_gbr.predict(X_val)\n",
    "rmse = mean_squared_error(y_val, predictions) ** 0.5\n",
    "print(\"Validation Root Mean Squared Error: \", rmse)\n",
    "\n",
    "predictions = clf_gbr.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions) ** 0.5\n",
    "print(\"Testing Root Mean Squared Error: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>SALEPRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149389.136620</td>\n",
       "      <td>150000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134155.530781</td>\n",
       "      <td>135000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167835.492799</td>\n",
       "      <td>170000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63845.719346</td>\n",
       "      <td>68000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16317.094826</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>118843.495134</td>\n",
       "      <td>123000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56824.994199</td>\n",
       "      <td>57500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62610.119090</td>\n",
       "      <td>63000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>171785.787937</td>\n",
       "      <td>173900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>158113.784320</td>\n",
       "      <td>160000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>106606.401506</td>\n",
       "      <td>105000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>146150.614000</td>\n",
       "      <td>149000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>150193.663519</td>\n",
       "      <td>139900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>201812.688609</td>\n",
       "      <td>190000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>157228.566720</td>\n",
       "      <td>160000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  SALEPRICE\n",
       "0   149389.136620   150000.0\n",
       "1   134155.530781   135000.0\n",
       "2   167835.492799   170000.0\n",
       "3    63845.719346    68000.0\n",
       "4    16317.094826    15000.0\n",
       "5   118843.495134   123000.0\n",
       "6    56824.994199    57500.0\n",
       "7    62610.119090    63000.0\n",
       "8   171785.787937   173900.0\n",
       "9   158113.784320   160000.0\n",
       "10  106606.401506   105000.0\n",
       "11  146150.614000   149000.0\n",
       "12  150193.663519   139900.0\n",
       "13  201812.688609   190000.0\n",
       "14  157228.566720   160000.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test.reset_index(drop=True)\n",
    "pd.concat([pd.Series(predictions), y_test],axis=1).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Model Builds Results\n",
    "\n",
    "We tested the following models:\n",
    "* KNearestNeighbors\n",
    "* GradientBoostedRegressor\n",
    "* Linear Support Vector Machine \n",
    "\n",
    "The best performing model was the `GradientBoostedRegressor`. Once we found the best performing model, we implemented a `Randomized Grid Search` which aimed to explore the grid randomly and find the best parameters. Although these results won't be perfect, they will be good. Once we found a set of 'good' parameters, we tested our predictions on our Training, Validation, and Testing set.\n",
    "\n",
    "Our testing set had a RMSE of `4780.84`. This indicates that we still have a lot of room for improvements, so we might want to attempt a different approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network using PyTorch\n",
    "\n",
    "The GradientBoostedRegressor didn't perform very well, so we are opting to try out a `Neural Network` using  `PyTorch`. Neural Networks are good at understanding patterns in raw data, so we might see better a perfomance here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network\n",
    "\n",
    "PyTorch allows you to create a python class that extends the torch.nn.Module class. This allows us to define a Neural Network.\n",
    "\n",
    "This example was inspired from the following source:\n",
    "https://medium.com/@benjamin.phillips22/simple-regression-with-neural-networks-in-pytorch-313f06910379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this is one way to define a network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the Data\n",
    "The data is currently in a Pandas DataFrame. We want to convert this data to a Tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_tensor(df):\n",
    "    \"\"\"Converts a pandas DataFrame object to a PyTorch tensor object\"\"\"\n",
    "    return torch.tensor(df.values)\n",
    "\n",
    "# Format data correctly.\n",
    "X_train_tensor = df_to_tensor(X_train).float()\n",
    "Y_train_tensor = df_to_tensor(y_train).float()\n",
    "Y_train_tensor = Y_train_tensor.reshape(Y_train_tensor.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Neural Network\n",
    "\n",
    "Here, we will train the Neural Network.\n",
    "\n",
    "We are using `100 Hidden Units`.\n",
    "\n",
    "We are choosing to use the `Adam` optimizer and choose to minimize the `Mean Squared Error`.\n",
    "\n",
    "We will train the network using `200` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:  36.2454617023468\n"
     ]
    }
   ],
   "source": [
    "# Instantiate PyTorch Neural Network\n",
    "net = Net(n_feature=X_train_tensor.shape[1], n_hidden=100, n_output=1)     # define the network\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# train the network\n",
    "for t in range(200):\n",
    "  \n",
    "    prediction = net(X_train_tensor)     # input x and predict based on x\n",
    "    \n",
    "    loss = loss_func(prediction, Y_train_tensor)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "\n",
    "print(\"Time Elapsed: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Root Mean Squared Error:  161307.5034222436\n",
      "Validation Root Mean Squared Error:  160757.96478130174\n",
      "Testing Root Mean Squared Error:  160370.69562488448\n"
     ]
    }
   ],
   "source": [
    "# Training Predictions\n",
    "X_train_tensor = df_to_tensor(X_train).float()\n",
    "\n",
    "train_prediction = net(X_train_tensor)\n",
    "train_prediction = train_prediction.data.numpy().reshape(train_prediction.shape[0])\n",
    "\n",
    "train_rmse = mean_squared_error(y_train, train_prediction) ** 0.5\n",
    "print(\"Training Root Mean Squared Error: \", train_rmse)\n",
    "\n",
    "# Validation Predictions\n",
    "X_val_tensor = df_to_tensor(X_val).float()\n",
    "\n",
    "val_prediction = net(X_val_tensor)\n",
    "val_prediction = val_prediction.data.numpy().reshape(val_prediction.shape[0])\n",
    "\n",
    "val_rmse = mean_squared_error(y_val, val_prediction) ** 0.5\n",
    "print(\"Validation Root Mean Squared Error: \", val_rmse)\n",
    "\n",
    "# Testing Predictions\n",
    "X_test_tensor = df_to_tensor(X_test).float()\n",
    "\n",
    "test_prediction = net(X_test_tensor)\n",
    "test_prediction = test_prediction.data.numpy().reshape(test_prediction.shape[0])\n",
    "\n",
    "test_rmse = mean_squared_error(y_test, test_prediction) ** 0.5\n",
    "print(\"Testing Root Mean Squared Error: \", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Results\n",
    "\n",
    "We seem to get lucky, as the `RMSE` is a lot worse on the Training and Validation data. We can see that our Testing RMSE performs **better** than the `GradientBoostedRegressor`, so if we had to accept one of the models, we might choose the Neural Netowrk.\n",
    "\n",
    "However, these results are still really poor. We should look to improve these results more by:\n",
    "* Cleaning our data set more\n",
    "* Trying different Standard Models\n",
    "* Playing around with the Neural Network settings more"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
