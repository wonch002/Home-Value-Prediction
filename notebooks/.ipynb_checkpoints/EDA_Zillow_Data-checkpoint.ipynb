{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.26.245.49:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21a6258ed08>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "* Full list of real estate properties in three counties (Los Angeles, Orange, and Ventura, California). \n",
    "* All training transactions occurred before October 15, 2016 (and a few after Oct 15, 2016).\n",
    "* Testing data are tranactions between October 15 and December 31, 2016.\n",
    "\n",
    "### Objective\n",
    "We are supposed to predict 6 time points for all properties (October 2016 (201610), November 2016 (201611), December 2016 (201612), October 2017 (201710), November 2017 (201711), and December 2017 (201712).\n",
    "\n",
    "Our target variable is the log-error between their Zestimate and the actual sale price. \n",
    "\n",
    "`logerror = log(Zestimate) - log(SalePrice)`\n",
    "\n",
    "This is given to us in the train.csv file.\n",
    "\n",
    "### Where can we find the data?\n",
    "\n",
    "\n",
    "Download the data from https://www.kaggle.com/c/zillow-prize-1/data and extract it into a folder called `zillow-prize-1`. This folder should be in the base level directory (the same directory as the .gitignore file)\n",
    "\n",
    "All features can be found in `properties_2016.csv`\n",
    "\n",
    "Targets are found in `train_2016.csv` and `train_2017.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_location = \"../zillow-prize-1/properties_2017.csv\"\n",
    "train1_location = \"../zillow-prize-1/train_2016_v2.csv\"\n",
    "train2_location = \"../zillow-prize-1/train_2017.csv\"\n",
    "test_location = \"../zillow-prize-1/sample_submission.csv\"\n",
    "\n",
    "\n",
    "features = spark.read.csv(features_location, header=True)\n",
    "training1 = spark.read.csv(train1_location, header=True)\n",
    "training2 = spark.read.csv(train2_location, header=True)\n",
    "testing = spark.read.csv(test_location, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does our features look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+------------------------+------------+-----------+----------+-------------------+---------------------+-----------------+----------+------------------------+----------------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-----+------------+-----------+------------+---------------+--------------+---------------------+--------+----------+-----------------+-------+-----------+------------+-----------+-----------+-------------------------+---------------------+------------------+----------------------+------------+--------------+--------------------+-----------+-------+-----------+-------------------+----------------------+-------+------------------+------------------+---------+---------------+-------------+--------------------------+-----------------+--------------+---------------------+---------+------------------+------------------+-------------------+\n",
      "|parcelid|airconditioningtypeid|architecturalstyletypeid|basementsqft|bathroomcnt|bedroomcnt|buildingclasstypeid|buildingqualitytypeid|calculatedbathnbr|decktypeid|finishedfloor1squarefeet|calculatedfinishedsquarefeet|finishedsquarefeet12|finishedsquarefeet13|finishedsquarefeet15|finishedsquarefeet50|finishedsquarefeet6| fips|fireplacecnt|fullbathcnt|garagecarcnt|garagetotalsqft|hashottuborspa|heatingorsystemtypeid|latitude| longitude|lotsizesquarefeet|poolcnt|poolsizesum|pooltypeid10|pooltypeid2|pooltypeid7|propertycountylandusecode|propertylandusetypeid|propertyzoningdesc|rawcensustractandblock|regionidcity|regionidcounty|regionidneighborhood|regionidzip|roomcnt|storytypeid|threequarterbathnbr|typeconstructiontypeid|unitcnt|yardbuildingsqft17|yardbuildingsqft26|yearbuilt|numberofstories|fireplaceflag|structuretaxvaluedollarcnt|taxvaluedollarcnt|assessmentyear|landtaxvaluedollarcnt|taxamount|taxdelinquencyflag|taxdelinquencyyear|censustractandblock|\n",
      "+--------+---------------------+------------------------+------------+-----------+----------+-------------------+---------------------+-----------------+----------+------------------------+----------------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-----+------------+-----------+------------+---------------+--------------+---------------------+--------+----------+-----------------+-------+-----------+------------+-----------+-----------+-------------------------+---------------------+------------------+----------------------+------------+--------------+--------------------+-----------+-------+-----------+-------------------+----------------------+-------+------------------+------------------+---------+---------------+-------------+--------------------------+-----------------+--------------+---------------------+---------+------------------+------------------+-------------------+\n",
      "|10754147|                 null|                    null|        null|        0.0|       0.0|               null|                 null|             null|      null|                    null|                        null|                null|                null|                null|                null|               null|06037|        null|       null|        null|           null|          null|                 null|34144442|-118654084|          85768.0|   null|       null|        null|       null|       null|                     010D|                  269|              null|        060378002.0410|       37688|          3101|                null|      96337|    0.0|       null|               null|                  null|   null|              null|              null|     null|           null|         null|                      null|              9.0|          2016|                  9.0|     null|              null|              null|               null|\n",
      "+--------+---------------------+------------------------+------------+-----------+----------+-------------------+---------------------+-----------------+----------+------------------------+----------------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-----+------------+-----------+------------+---------------+--------------+---------------------+--------+----------+-----------------+-------+-----------+------------+-----------+-----------+-------------------------+---------------------+------------------+----------------------+------------+--------------+--------------------+-----------+-------+-----------+-------------------+----------------------+-------+------------------+------------------+---------+---------------+-------------+--------------------------+-----------------+--------------+---------------------+---------+------------------+------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features.show(1) # Spark sucks wtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2985217, 58)\n"
     ]
    }
   ],
   "source": [
    "# What is the size of our DataFrame?\n",
    "print((features.count(), len(features.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of our data is 29.8 million rows and 58 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parcelid',\n",
       " 'airconditioningtypeid',\n",
       " 'architecturalstyletypeid',\n",
       " 'basementsqft',\n",
       " 'bathroomcnt',\n",
       " 'bedroomcnt',\n",
       " 'buildingclasstypeid',\n",
       " 'buildingqualitytypeid',\n",
       " 'calculatedbathnbr',\n",
       " 'decktypeid',\n",
       " 'finishedfloor1squarefeet',\n",
       " 'calculatedfinishedsquarefeet',\n",
       " 'finishedsquarefeet12',\n",
       " 'finishedsquarefeet13',\n",
       " 'finishedsquarefeet15',\n",
       " 'finishedsquarefeet50',\n",
       " 'finishedsquarefeet6',\n",
       " 'fips',\n",
       " 'fireplacecnt',\n",
       " 'fullbathcnt',\n",
       " 'garagecarcnt',\n",
       " 'garagetotalsqft',\n",
       " 'hashottuborspa',\n",
       " 'heatingorsystemtypeid',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'lotsizesquarefeet',\n",
       " 'poolcnt',\n",
       " 'poolsizesum',\n",
       " 'pooltypeid10',\n",
       " 'pooltypeid2',\n",
       " 'pooltypeid7',\n",
       " 'propertycountylandusecode',\n",
       " 'propertylandusetypeid',\n",
       " 'propertyzoningdesc',\n",
       " 'rawcensustractandblock',\n",
       " 'regionidcity',\n",
       " 'regionidcounty',\n",
       " 'regionidneighborhood',\n",
       " 'regionidzip',\n",
       " 'roomcnt',\n",
       " 'storytypeid',\n",
       " 'threequarterbathnbr',\n",
       " 'typeconstructiontypeid',\n",
       " 'unitcnt',\n",
       " 'yardbuildingsqft17',\n",
       " 'yardbuildingsqft26',\n",
       " 'yearbuilt',\n",
       " 'numberofstories',\n",
       " 'fireplaceflag',\n",
       " 'structuretaxvaluedollarcnt',\n",
       " 'taxvaluedollarcnt',\n",
       " 'assessmentyear',\n",
       " 'landtaxvaluedollarcnt',\n",
       " 'taxamount',\n",
       " 'taxdelinquencyflag',\n",
       " 'taxdelinquencyyear',\n",
       " 'censustractandblock']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see all of the rows.\n",
    "features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does our training set look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatentate 2016 and 2017\n",
    "training = training1.union(training2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+\n",
      "|parcelid|logerror|transactiondate|\n",
      "+--------+--------+---------------+\n",
      "|11016594|  0.0276|     2016-01-01|\n",
      "|14366692| -0.1684|     2016-01-01|\n",
      "|12098116|  -0.004|     2016-01-01|\n",
      "+--------+--------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(3) # Spark sucks wtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167888, 3)\n",
      "2016 Count:  (90275, 3)\n",
      "2017 Count:  (77613, 3)\n"
     ]
    }
   ],
   "source": [
    "# What is the size of our DataFrame?\n",
    "print((training.count(), len(training.columns)))\n",
    "print(\"2016 Count: \", (training1.count(), len(training1.columns)))\n",
    "print(\"2017 Count: \", (training2.count(), len(training2.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parcelid', 'logerror', 'transactiondate']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see all of the rows.\n",
    "training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to note about training data\n",
    "We can join the features with the parcelid. \n",
    "\n",
    "We are predicting the value of 'logerror'.\n",
    "\n",
    "We will want to potentially implement timeseries of this (with transactiondate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does our testing set look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+------+------+------+------+\n",
      "|ParcelId|201610|201611|201612|201710|201711|201712|\n",
      "+--------+------+------+------+------+------+------+\n",
      "|10754147|     0|     0|     0|     0|     0|     0|\n",
      "|10759547|     0|     0|     0|     0|     0|     0|\n",
      "|10843547|     0|     0|     0|     0|     0|     0|\n",
      "+--------+------+------+------+------+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing.show(3) # Spark sucks wtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2985217, 7)\n"
     ]
    }
   ],
   "source": [
    "# What is the size of our DataFrame?\n",
    "print((testing.count(), len(testing.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would want to exclude the testing data. I think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ParcelId', '201610', '201611', '201612', '201710', '201711', '201712']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see all of the rows.\n",
    "testing.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to note about the testing set\n",
    "1. We will want to exclude all things in the training set. \n",
    "2. The ideal `zscore` is 0. \n",
    "    * I'm not 100% sure what this means for us...\n",
    "        * Do we just try to predict the `zscore` in the training set?\n",
    "  \n",
    "I'm not sure if this is actually our testing set... hmm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
